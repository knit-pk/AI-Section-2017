{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA - State–Action–Reward–State–Action\n",
    "\n",
    "$$\n",
    "Q(s_t,a_t) = Q(s_t,a_t) + \\alpha (r + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t,a_t))\n",
    "$$\n",
    "where<br>\n",
    "$ \\alpha $ - step size (learning rate)<br>\n",
    "$ \\gamma $ - discount factor<br>\n",
    "$ s_t $ - current state<br>\n",
    "$ s_{t+1} $ - next state <br>\n",
    "$ r $ - reward<br>\n",
    "$ a $ - action\n",
    "\n",
    "The Q-value depends on the current state of the agent $s_t$, the action the agent chooses $a_t$, the reward $r$ the agent gets for choosing this action, the state $s_{t+1}$ that the agent will now be in after taking that action, and finally the next action $a_{t+1}$ the agent will choose in its new state.\n",
    "\n",
    "### Comparing Q-learning and SARSA\n",
    "SARSA\n",
    "$$\n",
    "Q(s_t,a_t) = Q(s_t,a_t) + \\alpha (r + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t,a_t))\n",
    "$$\n",
    "Q-learning\n",
    "$$\n",
    "Q(s_t,a_t) = Q(s_t,a_t) + \\alpha (r + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t,a_t))\n",
    "$$\n",
    "\n",
    "### Example - 2000 episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last visited states and actions:\n",
      "[['0' '0' '0' '4' '4' '8' '8' '8' '9' '8' '9' '13' '13' '14']\n",
      " ['l' 'l' 'l' 'l' 'u' 'u' 'u' 'd' 'u' 'd' 'r' 'r' 'u' '-']]\n",
      "\n",
      "Last move:\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "\n",
      "Numbers representing states:\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n",
      "\n",
      "Q-table:\n",
      "[[  9.84857068e-02   3.81064777e-04   4.25934352e-04   2.71681067e-04]\n",
      " [  1.55885969e-03   1.20787031e-04   8.49505224e-05   9.99091620e-02]\n",
      " [  1.80585779e-01   2.89716854e-05   4.10769756e-04   3.77641859e-04]\n",
      " [  5.89198625e-06   1.21873822e-04   6.71749217e-05   2.16134593e-06]\n",
      " [  6.37015446e-02   5.20212186e-04   1.99713651e-04   1.33293168e-03]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  3.47444063e-06   2.49835288e-09   1.87561714e-04   7.91550829e-07]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  2.62351523e-04   6.60937443e-05   2.58062127e-05   7.90674016e-02]\n",
      " [  2.11931543e-04   5.88325406e-01   1.11575138e-03   1.09658886e-03]\n",
      " [  6.03355705e-01   2.50445246e-04   8.06735774e-04   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  4.94398045e-05   3.47205556e-05   8.47419994e-01   1.08750438e-03]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   9.84983895e-01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "a = .8 #alpha\n",
    "y = .95 #gamma\n",
    "num_episodes = 2000\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    visited_states = [0, ]\n",
    "    choosed_actions = []\n",
    "    \n",
    "    current_state = env.reset()\n",
    "    current_action = np.argmax(Q[current_state,:])\n",
    "    for j in range(100):\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(current_action)\n",
    "        \n",
    "        next_action = np.argmax(Q[next_state,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        \n",
    "        Q[current_state, current_action] += a*(reward + y*Q[next_state, next_action] - Q[current_state, current_action])\n",
    "        \n",
    "        visited_states.append(current_state)\n",
    "        choosed_actions.append(\n",
    "        {\n",
    "            0 : 'l',\n",
    "            1 : 'd',\n",
    "            2 : 'r',\n",
    "            3 : 'u'\n",
    "        }[current_action])\n",
    "        \n",
    "        current_state = next_state\n",
    "        current_action = next_action\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "choosed_actions.append('-')\n",
    "print('Last visited states and actions:')\n",
    "print(np.array([visited_states, choosed_actions]))\n",
    "print()\n",
    "print('Last move:')\n",
    "env.render()\n",
    "print()\n",
    "print('Numbers representing states:')\n",
    "print(np.arange(0,16).reshape(4,4))\n",
    "print()\n",
    "print('Q-table:')\n",
    "print(Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
