{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA - State–Action–Reward–State–Action\n",
    "\n",
    "$$\n",
    "Q(s_t,a_t) = Q(s_t,a_t) + \\alpha (r + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t,a_t))\n",
    "$$\n",
    "where<br>\n",
    "$ \\alpha $ - step size (learning rate)<br>\n",
    "$ \\gamma $ - discount factor<br>\n",
    "$ s_t $ - current state<br>\n",
    "$ s_{t+1} $ - next state <br>\n",
    "$ r $ - reward<br>\n",
    "$ a $ - action\n",
    "\n",
    "The Q-value depends on the current state of the agent $s_t$, the action the agent chooses $a_t$, the reward $r$ the agent gets for choosing this action, the state $s_{t+1}$ that the agent will now be in after taking that action, and finally the next action $a_{t+1}$ the agent will choose in its new state.\n",
    "\n",
    "### Comparing Q-learning and SARSA\n",
    "SARSA\n",
    "$$\n",
    "Q(s_t,a_t) = Q(s_t,a_t) + \\alpha (r + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t,a_t))\n",
    "$$\n",
    "Q-learning\n",
    "$$\n",
    "Q(s_t,a_t) = Q(s_t,a_t) + \\alpha (r + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t,a_t))\n",
    "$$\n",
    "\n",
    "The major difference between SARSA and Q-Learning, is that the maximum reward for the next state is not necessarily used for updating the Q-values. Instead, a new action, and therefore reward, is selected using the same policy that determined the original action.\n",
    "\n",
    "### Example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table:\n",
      "[[  2.96856986e-01   2.40164072e-04   5.95337846e-04   4.86136204e-04]\n",
      " [  1.52752606e-05   1.16452456e-03   7.23610020e-04   6.56125509e-02]\n",
      " [  1.72521667e-04   7.07783616e-02   1.83614554e-04   8.31396591e-04]\n",
      " [  5.09110558e-06   4.00247793e-08   1.20667714e-04   6.38800679e-02]\n",
      " [  1.99064756e-01   1.17901438e-03   3.53596104e-04   9.98988802e-05]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  1.71808538e-05   1.04510488e-04   1.50176370e-02   1.35763732e-08]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  2.09330533e-04   1.29270101e-04   4.62964037e-04   1.03072295e-01]\n",
      " [  0.00000000e+00   5.52217798e-01   1.78927846e-03   3.71852679e-05]\n",
      " [  7.49010963e-02   9.44015147e-05   0.00000000e+00   2.68498075e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   3.43941442e-01   4.28438328e-05]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   8.15572090e-01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "a = .8 #alpha\n",
    "y = .95 #gamma\n",
    "num_episodes = 2000\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    current_state = env.reset()\n",
    "    current_action = np.argmax(Q[current_state,:])\n",
    "    for j in range(100):\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(current_action)\n",
    "        \n",
    "        next_action = np.argmax(Q[next_state,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        \n",
    "        Q[current_state, current_action] += a*(reward + y*Q[next_state, next_action] - Q[current_state, current_action])\n",
    "        \n",
    "        current_state = next_state\n",
    "        current_action = next_action\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "print('Q-table:')\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last visited states and actions:\n",
      "[['0' '0' '0' '0' '0' '4' '4' '8' '4' '8' '9' '13' '14']\n",
      " ['l' 'l' 'l' 'l' 'l' 'l' 'u' 'l' 'u' 'd' 'r' 'r' '-']]\n",
      "\n",
      "Last move:\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "\n",
      "Numbers representing states:\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n",
      "\n",
      "Q-table:\n",
      "[[  4.64472731e-01   3.61962284e-05   1.43738625e-04   7.08616356e-05]\n",
      " [  1.31054962e-04   7.61097995e-04   2.12605914e-04   1.72647706e-01]\n",
      " [  2.06265009e-04   2.34825148e-01   7.40766980e-04   7.07559250e-04]\n",
      " [  6.37087438e-05   9.32407378e-04   1.56706556e-04   1.10325793e-01]\n",
      " [  6.33070719e-01   1.03994546e-04   2.17606301e-05   1.00200350e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  5.50441975e-05   6.23260959e-08   3.11330846e-02   1.97457558e-07]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  4.73826368e-04   1.01255101e-03   1.17546810e-03   7.56133544e-01]\n",
      " [  2.25749240e-03   5.42136319e-01   1.11496939e-03   6.02467326e-07]\n",
      " [  1.38233774e-01   2.37917152e-04   9.26731040e-04   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   2.21784295e-01   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   8.32205446e-01   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "a = .8 #alpha\n",
    "y = .95 #gamma\n",
    "num_episodes = 2000\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    visited_states = [0, ]\n",
    "    choosed_actions = []\n",
    "    \n",
    "    current_state = env.reset()\n",
    "    current_action = np.argmax(Q[current_state,:])\n",
    "    for j in range(100):\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(current_action)\n",
    "        \n",
    "        next_action = np.argmax(Q[next_state,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        \n",
    "        Q[current_state, current_action] += a*(reward + y*Q[next_state, next_action] - Q[current_state, current_action])\n",
    "        \n",
    "        visited_states.append(current_state)\n",
    "        choosed_actions.append(\n",
    "        {\n",
    "            0 : 'l',\n",
    "            1 : 'd',\n",
    "            2 : 'r',\n",
    "            3 : 'u'\n",
    "        }[current_action])\n",
    "        \n",
    "        current_state = next_state\n",
    "        current_action = next_action\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "choosed_actions.append('-')\n",
    "print('Last visited states and actions:')\n",
    "print(np.array([visited_states, choosed_actions]))\n",
    "print()\n",
    "print('Last move:')\n",
    "env.render()\n",
    "print()\n",
    "print('Numbers representing states:')\n",
    "print(np.arange(0,16).reshape(4,4))\n",
    "print()\n",
    "print('Q-table:')\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- http://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html\n",
    "- https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
