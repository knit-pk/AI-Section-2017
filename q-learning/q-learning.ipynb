{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Table learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Q-Table learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal difference: (metoda różnic czasowych)\n",
    "$$\n",
    "Q(s_t,a_t) = Q(s_t,a_t) + \\alpha (r + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t,a_t))\n",
    "$$\n",
    "where<br>\n",
    "$ \\alpha $ - step size (learning rate)<br>\n",
    "$ \\gamma $ - discount factor<br>\n",
    "$ s_t $ - current state<br>\n",
    "$ s_{t+1} $ - next state <br>\n",
    "$ r $ - reward<br>\n",
    "$ a $ - action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 4)\n"
     ]
    }
   ],
   "source": [
    "#Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "#size of Q table\n",
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "a = .8 #alpha\n",
    "y = .95 #gamma\n",
    "\n",
    "#reset environment, state = 0\n",
    "state = env.reset()\n",
    "    \n",
    "reward = 0\n",
    "done = False\n",
    "for j in range(100):\n",
    "        \n",
    "    #choose action by greedily (with noise) picking from Q table (max value of current state)\n",
    "    action = np.argmax(Q[state,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "\n",
    "    #get new state and reward from environment\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "    #update Q-Table with new knowledge\n",
    "    Q[state, action] = Q[state, action] + a*(reward + y*np.max(Q[next_state,:]) - Q[state, action])\n",
    "        \n",
    "    #update current state\n",
    "    state = next_state\n",
    "       \n",
    "    if done == True:\n",
    "        break\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - one episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visited states and actions:\n",
      "[['0' '4' '4' '0' '4' '4' '4' '5']\n",
      " ['r' 'l' 'u' 'd' 'u' 'd' 'r' '-']]\n",
      "\n",
      "Last move:\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Numbers representing states:\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n",
      "\n",
      "Total reward:  0.0\n"
     ]
    }
   ],
   "source": [
    "a = .8 #alpha\n",
    "y = .95 #gamma\n",
    "num_episodes = 1\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    #reset environment, state = 0\n",
    "    state = env.reset()\n",
    "    \n",
    "    reward = 0\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    visited_states = [0, ]\n",
    "    choosed_actions = []\n",
    "    for j in range(100):\n",
    "        \n",
    "        #choose action by greedily (with noise) picking from Q table (max value of current state)\n",
    "        action = np.argmax(Q[state,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        \n",
    "        #get new state and reward from environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        #update Q-Table with new knowledge\n",
    "        Q[state, action] = Q[state, action] + a*(reward + y*np.max(Q[next_state,:]) - Q[state, action])\n",
    "        \n",
    "        #update current state\n",
    "        state = next_state\n",
    "        \n",
    "        visited_states.append(state)\n",
    "        choosed_actions.append(\n",
    "        {\n",
    "            0 : 'l',\n",
    "            1 : 'd',\n",
    "            2 : 'r',\n",
    "            3 : 'u'\n",
    "        }[action])\n",
    "        total_reward += reward        \n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "choosed_actions.append('-')\n",
    "print('Visited states and actions:')\n",
    "print(np.array([visited_states, choosed_actions]))\n",
    "print()\n",
    "print('Last move:')\n",
    "env.render()\n",
    "print()\n",
    "print('Numbers representing states:')\n",
    "print(np.arange(0,16).reshape(4,4))\n",
    "print()\n",
    "print('Total reward: ',total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add noise when choosing action, because in this environment we don't have penalties for jumping into the hole (H state). Without noise the agent would always choose to go left (np.argmax() returns first occurance of the max value, so at the beginning when all values for states are zeros [0, 0, 0, 0], it returns index 0 all the time - which corresponds to going left)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - 2000 episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.5715\n",
      "[[  1.15803385e-01   5.29472035e-03   6.97934648e-03   6.35791426e-03]\n",
      " [  1.47652364e-03   1.48728690e-03   3.43557700e-04   3.57391445e-01]\n",
      " [  1.94310635e-03   1.48677662e-01   1.31650910e-03   5.07663444e-03]\n",
      " [  9.53837315e-04   1.67282445e-03   9.26841364e-04   1.19650849e-01]\n",
      " [  2.86314788e-01   1.18328792e-03   2.00981015e-03   2.07310092e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  2.64796467e-02   1.59111901e-04   2.85943168e-04   5.19353776e-06]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  1.01119728e-03   1.03306913e-04   7.81959077e-05   2.38798460e-01]\n",
      " [  1.92274132e-03   3.79814320e-01   2.31781966e-04   3.22077987e-04]\n",
      " [  6.78737372e-01   1.70064783e-04   2.70120921e-04   5.81185787e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   8.20907884e-01   0.00000000e+00]\n",
      " [  4.09061547e-03   0.00000000e+00   9.86058761e-01   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "a = .8 #alpha\n",
    "y = .95 #gamma\n",
    "num_episodes = 2000\n",
    "rList = [] #reward list\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    reward = 0\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    for j in range(100):\n",
    "        action = np.argmax(Q[state,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        Q[state, action] = Q[state, action] + a*(reward + y*np.max(Q[next_state,:]) - Q[state, action])\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        total_reward += reward        \n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "    rList.append(total_reward)\n",
    "    \n",
    "print(\"Score over time: \" +  str(sum(rList)/num_episodes))\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy stuff\n",
    "### What Q[state, :] means?\n",
    "\n",
    "**Python slice notation**<br>\n",
    "Given an array \"a\":\n",
    "```python\n",
    "a[start:end] # items start through end-1\n",
    "a[start:]    # items start through the rest of the array\n",
    "a[:end]      # items from the beginning through end-1\n",
    "a[:]         # a copy of the whole array\n",
    "\n",
    "a[start:end:step] # start through not past end, by step\n",
    "```\n",
    "\n",
    "Let's consider we have a matrices like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [2 2 2]\n",
      " [3 3 3]]\n",
      "\n",
      "[[1 2 3]\n",
      " [1 2 3]\n",
      " [1 2 3]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 1, 1],[2, 2, 2],[3, 3, 3]])\n",
    "B = np.array([[1, 2, 3],[1, 2, 3],[1, 2, 3]])\n",
    "print(A)\n",
    "print()\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a \"slice\" of array means to get subarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#get first row\n",
    "subA = A[0, :]\n",
    "#first argument is row number - 1st row (0 indexed array)\n",
    "#second argument is column numbers - \":\" means every column\n",
    "print(subA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [1 2]\n",
      " [1 2]]\n"
     ]
    }
   ],
   "source": [
    "#get first two columns\n",
    "subB = B[:, 0:2]\n",
    "# 0:2 - means column [0, 2) - column 0 to 2, without 2\n",
    "print(subB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### np.argmax(a, axis=None, out=None)\n",
    "Returns the indices of the maximum values along an axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  3  2]\n",
      " [ 3  4 16]]\n"
     ]
    }
   ],
   "source": [
    "array = np.array([[1,3,2],[3,4,16]])\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 2]\n",
      "max value in first row on index:  1\n",
      "\n",
      "[ 3  4 16]\n",
      "max value in second row on index:  2\n"
     ]
    }
   ],
   "source": [
    "idx = np.argmax(array[0,:])\n",
    "print(array[0,:])\n",
    "print('max value in first row on index: ', idx)\n",
    "print()\n",
    "idx = np.argmax(array[1,:])\n",
    "print(array[1,:])\n",
    "print('max value in second row on index: ', idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
